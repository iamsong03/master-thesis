@article{Cybenko1989ApproximationBS,
  title={Approximation by superpositions of a sigmoidal function},
  author={George V. Cybenko},
  journal={Mathematics of Control, Signals and Systems},
  year={1989},
  volume={2},
  pages={303-314},
  url={https://api.semanticscholar.org/CorpusID:3958369}
}

@article{HORNIK1989359,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@article{LESHNO1993861,
title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
journal = {Neural Networks},
volume = {6},
number = {6},
pages = {861-867},
year = {1993},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(05)80131-5},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005801315},
author = {Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken},
keywords = {Multilayer feedforward networks, Activation functions, Role of threshold, Universal approximation capabilities, (Î¼) approximation},
abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.}
}

@Inbook{Maass1994,
author="Maass, W.
and Schnitger, G.
and Sontag, E. D.",
title="A Comparison of the Computational Power of Sigmoid and Boolean Threshold Circuits",
bookTitle="Theoretical Advances in Neural Computation and Learning",
year="1994",
publisher="Springer US",
address="Boston, MA",
pages="127--151",
abstract="Research on neural networks has led to the investigation of massively parallel computational models that consist of analog computational elements. Usually these analog computational elements are assumed to be smooth threshold gates, i.e. $\gamma$-gates for some nondecreasing differentiate function $\gamma$: â {\textrightarrow} â. A $\gamma$-gate with weights w1,{\ldots}, wmâ â; and threshold t â â is defined to be a gate that computes the function (x1,...,xm) {\$}{\$}{\backslash}mapsto {\backslash}gamma {\backslash}left( {\{}{\backslash}sum{\backslash}nolimits{\_}{\{}i = 1{\}}^m {\{}{\{}w{\_}i{\}}{\{}x{\_}i{\}} - t{\}} {\}} {\backslash}right){\$}{\$}from âminto â. A $\gamma$-circuit is defined as a directed acyclic circuit that consists of $\gamma$-gates. The most frequently considered special case of a smooth threshold circuit is the sigmoid threshold circuit, which is a $\sigma$-circuit for $\sigma$ â {\textrightarrow} â defined by {\$}{\$}{\backslash}sigma (x) = {\backslash}frac{\{}1{\}}{\{}{\{}1 + {\backslash}exp ( - x){\}}{\}}{\$}{\$}",
isbn="978-1-4615-2696-4",
doi="10.1007/978-1-4615-2696-4_4",
url="https://doi.org/10.1007/978-1-4615-2696-4_4"
}

@article{Montfar2013UniversalAD,
  title={Universal Approximation Depth and Errors of Narrow Belief Networks with Discrete Units},
  author={Guido Mont{\'u}far},
  journal={Neural Computation},
  year={2013},
  volume={26},
  pages={1386-1407},
  url={https://api.semanticscholar.org/CorpusID:9714964}
}

@article{Lin2016WhyDD,
  title={Why Does Deep and Cheap Learning Work So Well?},
  author={Henry W. Lin and Max Tegmark},
  journal={Journal of Statistical Physics},
  year={2016},
  volume={168},
  pages={1223-1247},
  url={https://api.semanticscholar.org/CorpusID:2494297}
}

@article{PhysRevResearch.1.033063,
  title = {Continuous-variable quantum neural networks},
  author = {Killoran, Nathan and Bromley, Thomas R. and Arrazola, Juan Miguel and Schuld, Maria and Quesada, Nicol\'as and Lloyd, Seth},
  journal = {Phys. Rev. Res.},
  volume = {1},
  issue = {3},
  pages = {033063},
  numpages = {22},
  year = {2019},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.1.033063},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.1.033063}
}

@misc{arrazola2017quantum,
      title={Quantum supremacy and high-dimensional integration}, 
      author={Juan Miguel Arrazola and Patrick Rebentrost and Christian Weedbrook},
      year={2017},
      eprint={1712.07288},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}