{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5a79c39-1e67-4fb0-b980-75a32efd4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qutip as qt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from qutip import Qobj, wigner\n",
    "import time\n",
    "import os\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d3451b-8efb-4bf9-80e3-5d3d1413286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def get_vacuum_state_tf(dim):\n",
    "    vacuum_state = qt.basis(dim, 0)\n",
    "    return tf.convert_to_tensor(vacuum_state.full(), dtype=tf.complex64)\n",
    "\n",
    "def tf_annihilation(dim):\n",
    "    diag_vals = tf.math.sqrt(tf.cast(tf.range(1, dim), dtype=tf.float32))\n",
    "    diag_vals = tf.cast(diag_vals, dtype=tf.complex64)\n",
    "    return tf.linalg.diag(diag_vals, k=1)\n",
    "\n",
    "def tf_number(dim):\n",
    "    diag_vals = tf.range(0.0, dim, dtype=tf.float32)\n",
    "    diag_vals = tf.cast(diag_vals, dtype=tf.complex64)\n",
    "    return tf.linalg.diag(diag_vals)\n",
    "\n",
    "def tf_displacement_operator(dim, alpha):\n",
    "    alpha = tf.cast(alpha, dtype=tf.complex64)\n",
    "    a = tf_annihilation(dim)\n",
    "    term1 = alpha * tf.linalg.adjoint(a)\n",
    "    term2 = tf.math.conj(alpha) * a\n",
    "    D = tf.linalg.expm(term1 - term2)\n",
    "    return D\n",
    "\n",
    "def tf_displacement_encoding(dim, alpha_vec):\n",
    "    alpha_vec = tf.cast(alpha_vec, dtype=tf.complex64)\n",
    "    num = tf.shape(alpha_vec)[0]\n",
    "    a = tf_annihilation(dim)\n",
    "    term1 = tf.linalg.adjoint(a)\n",
    "    term2 = a\n",
    "    term1_batch = tf.tile(tf.expand_dims(term1, 0), [num, 1, 1])\n",
    "    term2_batch = tf.tile(tf.expand_dims(term2, 0), [num, 1, 1])\n",
    "    alpha_vec = tf.reshape(alpha_vec, [-1, 1, 1])\n",
    "    D = tf.linalg.expm(alpha_vec * term1_batch - tf.math.conj(alpha_vec) * term2_batch)\n",
    "    return D\n",
    "\n",
    "def tf_rotation_operator(dim, theta):\n",
    "    theta = tf.cast(theta, dtype=tf.complex64)\n",
    "    n = tf_number(dim)\n",
    "    R = tf.linalg.expm(-1j * theta * n)\n",
    "    return R\n",
    "\n",
    "def tf_squeezing_operator(dim, r):\n",
    "    r = tf.cast(r, dtype=tf.complex64)\n",
    "    a = tf_annihilation(dim)\n",
    "    term1 = r * tf.linalg.adjoint(a) * a\n",
    "    term2 = tf.math.conj(r) * a * tf.linalg.adjoint(a)\n",
    "    S = tf.linalg.expm(0.5 * (term1 - term2))\n",
    "    return S\n",
    "\n",
    "def tf_kerr_operator(dim, kappa):\n",
    "    kappa = tf.cast(kappa, dtype=tf.complex64)\n",
    "    n = tf_number(dim)\n",
    "    K = tf.linalg.expm(1j * kappa * n * n)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19490851-611c-48bb-bab0-6a59fdd8daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Custom Layer for Quantum Encoding\n",
    "class QuantumEncodingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, vacuum_state, **kwargs):\n",
    "        super(QuantumEncodingLayer, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.vacuum_state = vacuum_state\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        batch_vacuum_state = tf.tile(tf.expand_dims(self.vacuum_state, axis=0), [batch_size, 1, 1])\n",
    "        batch_displacement_operators = tf_displacement_encoding(self.dim, inputs)\n",
    "        displaced_states = tf.einsum('bij,bjk->bik', batch_displacement_operators, batch_vacuum_state)\n",
    "        return displaced_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27be586d-cd9e-4029-a28a-c782ff8b18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Custom Layer for Quantum Transformations\n",
    "class QuantumLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, **kwargs):\n",
    "        super(QuantumLayer, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed = 42)\n",
    "        self.theta_1 = self.add_weight(\"theta_1\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        self.theta_2 = self.add_weight(\"theta_2\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        self.r = self.add_weight(\"r\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        self.b = self.add_weight(\"b\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        self.kappa = self.add_weight(\"kappa\", shape=[1,], initializer=initializer, trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        # Compute operator tensors dynamically based on the current trainable variables\n",
    "        D_tensor = tf.expand_dims(tf_displacement_operator(self.dim, self.b), 0)\n",
    "        R_tensor_1 = tf.expand_dims(tf_rotation_operator(self.dim, self.theta_1), 0)\n",
    "        S_tensor = tf.expand_dims(tf_squeezing_operator(self.dim, self.r), 0)\n",
    "        R_tensor_2 = tf.expand_dims(tf_rotation_operator(self.dim, self.theta_2), 0)\n",
    "        K_tensor = tf.expand_dims(tf_kerr_operator(self.dim, self.kappa), 0)\n",
    "\n",
    "        # Tile the operator tensors for batch processing\n",
    "        D_tensor = tf.tile(D_tensor, [batch_size, 1, 1])\n",
    "        R_tensor_1 = tf.tile(R_tensor_1, [batch_size, 1, 1])\n",
    "        S_tensor = tf.tile(S_tensor, [batch_size, 1, 1])\n",
    "        R_tensor_2 = tf.tile(R_tensor_2, [batch_size, 1, 1])\n",
    "        K_tensor = tf.tile(K_tensor, [batch_size, 1, 1])\n",
    "\n",
    "        transformed_state = tf.einsum('bij,bjk->bik', R_tensor_1, inputs)\n",
    "        transformed_state = tf.einsum('bij,bjk->bik', S_tensor, transformed_state)\n",
    "        transformed_state = tf.einsum('bij,bjk->bik', R_tensor_2, transformed_state)\n",
    "        transformed_state = tf.einsum('bij,bjk->bik', D_tensor, transformed_state)\n",
    "        \n",
    "        activated_state = tf.einsum('bij,bjk->bik', K_tensor, transformed_state)\n",
    "        return activated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c59edbb-efc3-4e98-b15a-ff17868c16c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Custom Layer for Quantum Decoding\n",
    "class QuantumDecodingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, **kwargs):\n",
    "        super(QuantumDecodingLayer, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.x_operator = self.build_x_operator()\n",
    "\n",
    "    def build_x_operator(self):\n",
    "        a = tf_annihilation(self.dim)\n",
    "        x_operator = (a + tf.linalg.adjoint(a)) / 2.0\n",
    "        x_operator = tf.expand_dims(x_operator, axis=0)  # Add batch dimension\n",
    "        return x_operator\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        batch_x_operator = tf.tile(self.x_operator, [batch_size, 1, 1])\n",
    "\n",
    "        # Step 1: Compute \\hat{O} | \\psi \\rangle for each state in the batch\n",
    "        operator_applied_state = tf.einsum('bij,bjk->bik', batch_x_operator, inputs)\n",
    "\n",
    "        # Take the complex conjugate of each state and adjust dimensions\n",
    "        conj_inputs = tf.math.conj(inputs)  # Shape: (batch_size, dim, 1)\n",
    "        conj_inputs_adj = tf.transpose(conj_inputs, perm=[0, 2, 1])  # Shape: (batch_size, 1, dim)\n",
    "\n",
    "        # Compute the expectation value (inner product) for each state in the batch\n",
    "        x_expectation = tf.einsum('bij,bjk->bi', conj_inputs_adj, operator_applied_state) \n",
    "        x_expectation = tf.squeeze(x_expectation, axis=-1)\n",
    "\n",
    "        return tf.cast(x_expectation, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5f171f13-a8f4-4550-8b51-4f647b27c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WignerNegativityCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, quantum_encoding_layer, dim, xvec, pvec, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.quantum_encoding_layer = quantum_encoding_layer\n",
    "        self.dim = dim\n",
    "        self.xvec = xvec\n",
    "        self.pvec = pvec\n",
    "        self.wigner_negativities = []\n",
    "\n",
    "    def compute_negativity(self, state_ket_array):\n",
    "        state_ket_reshaped = state_ket_array.squeeze()\n",
    "        state_ket = Qobj(state_ket_reshaped).unit()  # Convert to Qobj\n",
    "        wigner_func = wigner(state_ket, self.xvec, self.pvec)\n",
    "        dx = np.full((100,), 0.00403)\n",
    "        return np.sum(np.abs(wigner_func)) - 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        expectation_values = self.model.predict(self.xvec)  # Model's output\n",
    "        quantum_states = self.quantum_encoding_layer(expectation_values)  # Convert to quantum states\n",
    "        quantum_states_np = quantum_states.numpy()  # Convert TensorFlow tensor to NumPy array\n",
    "\n",
    "        # Prepare the states for parallel processing\n",
    "        state_kets_arrays = [quantum_states_np[i, :, 0] for i in range(quantum_states_np.shape[0])]\n",
    "\n",
    "        # Use ThreadPoolExecutor to parallelize the computation\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            negativities = list(executor.map(self.compute_negativity, state_kets_arrays))\n",
    "\n",
    "        # Average negativity over the batch\n",
    "        avg_negativity = np.mean(negativities)\n",
    "        self.wigner_negativities.append(avg_negativity)\n",
    "\n",
    "    def get_wigner_negativities(self):\n",
    "        return self.wigner_negativities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "549ac13c-c255-4839-833f-34687cdbc8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004027998939328379"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linspace(0, 2*np.pi, 100)\n",
    "np.diff(x)[0]*np.diff(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55b57004-09c8-43ca-9c18-d259fff59646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(models, histories, quantumness, configs, cutoff, x_data, y_data, y_data_noisy):\n",
    "    # Figure 1: 6 Fit Plots\n",
    "    fig1, axes1 = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes1 = axes1.flatten()\n",
    "    for i, model in enumerate(models):\n",
    "        y_pred = model.predict(x_data)\n",
    "        axes1[i].scatter(x_data, y_data_noisy, s=5, label='Noisy Data')\n",
    "        axes1[i].plot(x_data, y_pred, label='Fitted Curve', color='r')\n",
    "        axes1[i].plot(x_data, y_data, label='True Curve', color='g')\n",
    "        axes1[i].legend()\n",
    "        axes1[i].set_title(f'Model {i+1}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Figure 2: Loss History Plots\n",
    "    fig2, axes2 = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    # Loss for varying cutoff dimension\n",
    "    for i in range(len(cutoff)):\n",
    "        axes2[0].plot(histories[i].history['loss'], label=f'Cutoff: {cutoff[i]}')\n",
    "    axes2[0].set_title('Loss for Varying Cutoff Dimension')\n",
    "    axes2[0].set_xlabel('Epoch')\n",
    "    axes2[0].set_ylabel('MSE Loss')\n",
    "    axes2[0].legend()\n",
    "\n",
    "    # Loss for varying number of layers\n",
    "    for i in range(len(configs)):\n",
    "        axes2[1].plot(histories[i + len(cutoff)].history['loss'], label=f'Layers: {configs[i][0]}')\n",
    "    axes2[1].set_title('Loss for Varying Number of Layers')\n",
    "    axes2[1].set_xlabel('Epoch')\n",
    "    axes2[1].set_ylabel('MSE Loss')\n",
    "    axes2[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Figure 3: Loss and Validation Loss for Each Model\n",
    "    fig3, axes3 = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes3 = axes3.flatten()\n",
    "    for i in range(len(models)):\n",
    "        axes3[i].plot(histories[i].history['loss'], label='Training Loss')\n",
    "        axes3[i].plot(histories[i].history['val_loss'], label='Validation Loss')\n",
    "        axes3[i].set_title(f'Model {i+1} Loss vs Epochs')\n",
    "        axes3[i].set_xlabel('Epoch')\n",
    "        axes3[i].set_ylabel('MSE Loss')\n",
    "        axes3[i].legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Figure 4: Quantumness Plot for Each Model\n",
    "    fig4, axes4 = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes4 = axes4.flatten()\n",
    "    for i, quants in enumerate(quantumness):\n",
    "        axes4[i].plot(quants, label=f'Model {i+1} Quantumness')\n",
    "        axes4[i].set_title(f'Quantumness vs Epochs for Model {i+1}')\n",
    "        axes4[i].set_xlabel('Epoch')\n",
    "        axes4[i].set_ylabel('Average Wigner Negativity')\n",
    "        axes4[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save and show all figures\n",
    "    fig1.savefig('fit.png', facecolor='white')\n",
    "    fig2.savefig('mses.png', facecolor='white')\n",
    "    fig3.savefig('mse_with_val.png', facecolor='white')\n",
    "    fig4.savefig('quantumness.png', facecolor='white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9942576b-8cf9-40df-8784-2ef8239d5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training models with different configurations\n",
    "def train_models(x_data, y_data_noisy, cutoff_dim = [10], configs = [(6, 50)]):\n",
    "    trained_models = []\n",
    "    histories = []\n",
    "    quantumness = []\n",
    "    \n",
    "    if len(cutoff_dim) == 1:\n",
    "        for num_layers, epochs in configs:\n",
    "            # Create a new model for each configuration\n",
    "            vacuum_state = get_vacuum_state_tf(cutoff_dim[0])\n",
    "            model = tf.keras.Sequential([QuantumEncodingLayer(dim=cutoff_dim[0], vacuum_state=vacuum_state, name='QuantumEncoding')])\n",
    "            for i in range(num_layers):\n",
    "                model.add(QuantumLayer(dim=cutoff_dim[0], name=f'QuantumLayer_{i+1}'))\n",
    "            model.add(QuantumDecodingLayer(dim=cutoff_dim[0], name='QuantumDecoding'))\n",
    "    \n",
    "            # Compile and train the model\n",
    "            model.compile(optimizer='adam', loss='mse')\n",
    "            print(f'Training model with {num_layers} layers for {epochs} epochs...')\n",
    "            quantum_encoding_layer = QuantumEncodingLayer(dim=cutoff_dim[0], vacuum_state=get_vacuum_state_tf(cutoff_dim[0]))\n",
    "            wigner_callback = WignerNegativityCallback(quantum_encoding_layer, dim=cutoff_dim[0], xvec=x_data, pvec=x_data)\n",
    "            history = model.fit(x_data, y_data_noisy, validation_split=0.33, epochs=epochs, verbose=0, callbacks=wigner_callback)\n",
    "    \n",
    "            # Store the trained model and its history\n",
    "            histories.append(history)\n",
    "            trained_models.append(model)\n",
    "            quantumness.append(wigner_callback.get_wigner_negativities())\n",
    "    else:\n",
    "        for cutoff in cutoff_dim:\n",
    "            # Create a new model for each configuration\n",
    "            num_layers, epochs = configs[0]\n",
    "            vacuum_state = get_vacuum_state_tf(cutoff)\n",
    "            model = tf.keras.Sequential([QuantumEncodingLayer(dim=cutoff, vacuum_state=vacuum_state, name='QuantumEncoding')])\n",
    "            for i in range(num_layers):\n",
    "                model.add(QuantumLayer(dim=cutoff, name=f'QuantumLayer_{i+1}'))\n",
    "            model.add(QuantumDecodingLayer(dim=cutoff, name='QuantumDecoding'))\n",
    "    \n",
    "            # Compile and train the model\n",
    "            model.compile(optimizer='adam', loss='mse')\n",
    "            print(f'Training model with {num_layers} layers of {cutoff} dimensions...')\n",
    "            quantum_encoding_layer = QuantumEncodingLayer(dim=cutoff, vacuum_state=get_vacuum_state_tf(cutoff))\n",
    "            wigner_callback = WignerNegativityCallback(quantum_encoding_layer, dim=cutoff, xvec=x_data, pvec=x_data)\n",
    "            history = model.fit(x_data, y_data_noisy, validation_split=0.30, epochs=epochs, verbose=0, callbacks=wigner_callback)\n",
    "    \n",
    "            # Store the trained model and its history\n",
    "            histories.append(history)\n",
    "            trained_models.append(model)\n",
    "            quantumness.append(wigner_callback.get_wigner_negativities())\n",
    "\n",
    "    return trained_models, histories, quantumness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494587e8-0c8e-416c-b282-d014ffbcd93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 2 layers for 50 epochs...\n",
      "4/4 [==============================] - 2s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Training model with 4 layers for 50 epochs...\n"
     ]
    }
   ],
   "source": [
    "num_layers = 6\n",
    "cutoff_dim = [5, 10, 15] \n",
    "\n",
    "# Generate sine wave data\n",
    "num_points = 100\n",
    "x_data = np.linspace(0, 2 * np.pi, num_points)\n",
    "y_data = np.sin(x_data)\n",
    "\n",
    "# Add noise\n",
    "noise_level = 0.1\n",
    "y_data_noisy = y_data + noise_level * np.random.randn(num_points)\n",
    "\n",
    "# Reshape for the model\n",
    "x_data = x_data.astype(np.complex64).reshape(-1, 1)\n",
    "y_data_noisy = y_data_noisy.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "configs = [\n",
    "    (2, 50),\n",
    "    (4, 50),\n",
    "    (6, 50)\n",
    "]\n",
    "\n",
    "trained_models, histories, quantumness = train_models(x_data, y_data_noisy, configs = configs)\n",
    "trained_models2, histories2, quantumness2 = train_models(x_data, y_data_noisy, cutoff_dim = cutoff_dim)\n",
    "\n",
    "trained_models += trained_models2\n",
    "histories += histories2\n",
    "quantumness += quantumness2\n",
    "\n",
    "plot_results(trained_models, histories, quantumness, configs, cutoff_dim, x_data, y_data, y_data_noisy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
