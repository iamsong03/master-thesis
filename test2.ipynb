{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n  Mathematical Statistics\" (John Wiley, NY, 1950).\\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n  Structure and Classification Rule for Recognition in Partially Exposed\\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n  on Information Theory, May 1972, 431-433.\\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n  conceptual clustering system finds 3 classes in the data.\\n- Many, many more ...\\n\\n|details-end|\\n',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Use PCA to decrease the dimension of the Input to the quantum circuit\n",
    "n_features=3\n",
    "pca = sklearn.decomposition.PCA(n_components=n_features)\n",
    "pca.fit(data)\n",
    "data = pca.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt\n",
    "import qutip as qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 09:02:30.185576: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-02 09:02:30.205771: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-02 09:02:30.205783: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-02 09:02:30.206326: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-02 09:02:30.209743: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-02 09:02:30.946840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 09:02:32.007657: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-02 09:02:32.011518: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-02 09:02:32.011602: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-02 09:02:32.013451: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-02 09:02:32.013522: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-02 09:02:32.013626: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-02 09:02:32.050811: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-02 09:02:32.050879: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-02 09:02:32.050927: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-02 09:02:32.050967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21836 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#tf.config.run_functions_eagerly(False)\n",
    "\n",
    "# Enable GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qutip as qt\n",
    "\n",
    "# Utility functions\n",
    "def get_vacuum_state_tf(dim):\n",
    "    vacuum_state = qt.basis(dim, 0)\n",
    "    return tf.convert_to_tensor(vacuum_state.full(), dtype=tf.complex64)\n",
    "\n",
    "def annihilation(dim):\n",
    "    diag_vals = tf.math.sqrt(tf.cast(tf.range(1, dim), dtype=tf.complex64))\n",
    "    return tf.linalg.diag(diag_vals, k=1)\n",
    "\n",
    "def number(dim):\n",
    "    diag_vals = tf.cast(tf.range(0, dim), dtype=tf.complex64)\n",
    "    return tf.linalg.diag(diag_vals)\n",
    "\n",
    "def identity(dim):\n",
    "    return tf.linalg.LinearOperatorFullMatrix(tf.eye(dim, dtype=tf.complex64))\n",
    "\n",
    "def displacement_operator(dim, x, y=0):\n",
    "    x2 = tf.identity(x)\n",
    "    y2 = tf.identity(y)\n",
    "    alpha = tf.complex(x2, y2)\n",
    "    a = annihilation(dim)\n",
    "    term1 = alpha * tf.linalg.adjoint(a)\n",
    "    term2 = tf.math.conj(alpha) * a\n",
    "    D = tf.linalg.expm(term1 - term2)\n",
    "    return D\n",
    "\n",
    "def displacement_encoding(dim, alpha):\n",
    "    alpha = tf.cast(alpha, dtype=tf.complex64)\n",
    "    num, modes = tf.shape(alpha)\n",
    "    alpha_tens = tf.reshape(alpha, [num, modes, 1, 1])\n",
    "    a = annihilation(dim)\n",
    "    term1 = tf.linalg.adjoint(a)\n",
    "    term2 = a\n",
    "    term1_batch = tf.broadcast_to(term1, [num, modes, dim, dim])\n",
    "    term2_batch = tf.broadcast_to(term2, [num, modes, dim, dim])\n",
    "    D = tf.linalg.expm(alpha_tens * term1_batch - tf.math.conj(alpha_tens) * term2_batch)\n",
    "    return D\n",
    "\n",
    "def rotation_operator(dim, theta):\n",
    "    theta2 = tf.identity(theta)\n",
    "    theta2 = tf.cast(theta2, dtype=tf.complex64)\n",
    "    n = number(dim)\n",
    "    R = tf.linalg.expm(-1j * theta2 * n)\n",
    "    return tf.linalg.LinearOperatorFullMatrix(R)\n",
    "\n",
    "def squeezing_operator(dim, r):\n",
    "    r2 = tf.identity(r)\n",
    "    r2 = tf.cast(r2, dtype=tf.complex64)\n",
    "    a = annihilation(dim)\n",
    "    adag = tf.linalg.adjoint(a)\n",
    "    term1 = tf.math.conj(r2) * (a @ a)\n",
    "    term2 = r2 * (adag @ adag)\n",
    "    S = tf.linalg.expm(0.5 * (term1 - term2))\n",
    "    return S\n",
    "\n",
    "def kerr_operator(dim, kappa):\n",
    "    kappa2 = tf.identity(kappa)\n",
    "    kappa2 = tf.cast(kappa2, dtype=tf.complex64)\n",
    "    n = number(dim)\n",
    "    K = tf.linalg.expm(1j * kappa2 * (n @ n))\n",
    "\n",
    "    return K\n",
    "\n",
    "def cubic_phase_operator(dim, gamma):\n",
    "    a = annihilation(dim)\n",
    "    x = (a + tf.linalg.adjoint(a)) / 2.0\n",
    "    gamma2 = tf.identity(gamma)\n",
    "    gamma2 = tf.cast(gamma2, dtype=tf.complex64)\n",
    "    V = tf.linalg.expm(1j * (gamma2/3) * (x @ x @ x))\n",
    "\n",
    "    return V \n",
    "\n",
    "def phaseless_beamsplitter(dim, theta):\n",
    "    k = tf.shape(theta)[0]\n",
    "    num_modes = (1 + np.sqrt(1+8*k))/2\n",
    "    num_modes = tf.cast(num_modes, dtype=tf.int32)\n",
    "    theta = tf.identity(theta)\n",
    "    theta = tf.cast(theta, dtype=tf.complex64)\n",
    "    a = tf.linalg.LinearOperatorFullMatrix(annihilation(dim))\n",
    "    I = identity(dim)\n",
    "    a1 = tf.linalg.LinearOperatorKronecker([a, I])\n",
    "    a2 = tf.linalg.LinearOperatorKronecker([I, a])\n",
    "    matrix = a1.matmul(a2.to_dense(), adjoint_arg=True) - a1.matmul(a2.to_dense(), adjoint=True)\n",
    "    matrix = tf.broadcast_to(matrix, [num_modes, dim**2, dim**2])\n",
    "    theta = tf.reshape(theta, [k, 1, 1])\n",
    "    BS = tf.linalg.expm(theta*(matrix))\n",
    "    \n",
    "    return BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 25, 25), dtype=complex64, numpy=\n",
       "array([[[ 1.        +0.j,  0.        +0.j,  0.        +0.j, ...,\n",
       "          0.        +0.j,  0.        +0.j,  0.        +0.j],\n",
       "        [ 0.        +0.j,  0.5403023 +0.j,  0.        +0.j, ...,\n",
       "          0.        +0.j,  0.        +0.j,  0.        +0.j],\n",
       "        [ 0.        +0.j,  0.        +0.j,  0.29192656+0.j, ...,\n",
       "          0.        +0.j,  0.        +0.j,  0.        +0.j],\n",
       "        ...,\n",
       "        [ 0.        +0.j,  0.        +0.j,  0.        +0.j, ...,\n",
       "          0.5927532 +0.j,  0.        +0.j,  0.        +0.j],\n",
       "        [ 0.        +0.j,  0.        +0.j,  0.        +0.j, ...,\n",
       "          0.        +0.j, -0.6536437 -0.j,  0.        +0.j],\n",
       "        [ 0.        +0.j,  0.        +0.j,  0.        +0.j, ...,\n",
       "          0.        +0.j,  0.        +0.j,  1.        +0.j]],\n",
       "\n",
       "       [[ 1.        +0.j,  0.        +0.j,  0.        +0.j, ...,\n",
       "          0.        +0.j,  0.        +0.j,  0.        +0.j],\n",
       "        [ 0.        +0.j, -0.4161469 +0.j,  0.        +0.j, ...,\n",
       "          0.        +0.j,  0.        +0.j,  0.        +0.j],\n",
       "        [ 0.        +0.j,  0.        +0.j,  0.17317799+0.j, ...,\n",
       "          0.        +0.j,  0.        +0.j,  0.        +0.j],\n",
       "        ...,\n",
       "        [ 0.        +0.j,  0.        +0.j,  0.        +0.j, ...,\n",
       "          0.03441262+0.j,  0.        +0.j,  0.        +0.j],\n",
       "        [ 0.        +0.j,  0.        +0.j,  0.        +0.j, ...,\n",
       "          0.        +0.j, -0.14549968+0.j,  0.        +0.j],\n",
       "        [ 0.        +0.j,  0.        +0.j,  0.        +0.j, ...,\n",
       "          0.        +0.j,  0.        +0.j,  1.        +0.j]],\n",
       "\n",
       "       [[ 1.        +0.j,  0.        +0.j,  0.        +0.j, ...,\n",
       "          0.        +0.j,  0.        +0.j,  0.        +0.j],\n",
       "        [ 0.        +0.j, -0.9899924 +0.j,  0.        +0.j, ...,\n",
       "          0.        +0.j,  0.        +0.j,  0.        +0.j],\n",
       "        [ 0.        +0.j,  0.        +0.j,  0.9800851 +0.j, ...,\n",
       "          0.        +0.j,  0.        +0.j,  0.        +0.j],\n",
       "        ...,\n",
       "        [ 0.        +0.j,  0.        +0.j,  0.        +0.j, ...,\n",
       "          0.23450416+0.j,  0.        +0.j,  0.        +0.j],\n",
       "        [ 0.        +0.j,  0.        +0.j,  0.        +0.j, ...,\n",
       "          0.        +0.j,  0.8438539 +0.j,  0.        +0.j],\n",
       "        [ 0.        +0.j,  0.        +0.j,  0.        +0.j, ...,\n",
       "          0.        +0.j,  0.        +0.j,  1.        +0.j]]],\n",
       "      dtype=complex64)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_modes = 4\n",
    "batch_size = 10\n",
    "dim = 5\n",
    "\n",
    "phaseless_beamsplitter(dim, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interferometer(dim, params):\n",
    "    if num_modes%2 == 0:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Custom Layer for Quantum Encoding\n",
    "class QEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, vacuum_state, **kwargs):\n",
    "        super(QEncoder, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.vacuum_state = tf.cast(vacuum_state, dtype=tf.complex64)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size, num_modes = tf.shape(inputs)\n",
    "        squeezed_vacuum_state = tf.matmul(squeezing_operator(dim, 100.0), get_vacuum_state_tf(dim))\n",
    "        batch_squeezed_state = tf.broadcast_to(squeezed_vacuum_state, [batch_size, num_modes, dim, 1])\n",
    "        batch_displacement_operators = displacement_encoding(dim, inputs/2)\n",
    "        displaced_states = tf.einsum('bmij,bmjk->bmik', batch_displacement_operators, batch_squeezed_state)\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.abs(displaced_states)**2, axis=2, keepdims=True))\n",
    "        norm = tf.cast(norm, dtype=tf.complex64)\n",
    "        \n",
    "        return displaced_states/norm\n",
    "\n",
    "\n",
    "# TensorFlow Custom Layer for Quantum Transformations\n",
    "class QLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, stddev=0.05, tol=1e-9, activation='kerr', **kwargs):\n",
    "        super(QLayer, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.stddev = stddev\n",
    "        self.tol = tol\n",
    "        self.activation = activation.lower()\n",
    "        if self.activation not in ['kerr', 'cubicphase']:\n",
    "            raise ValueError(\"Activation must be either 'kerr' or 'cubicphase'\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=self.stddev, seed=42)\n",
    "        self.theta_1 = self.add_weight(\"theta_1\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        self.theta_2 = self.add_weight(\"theta_2\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        self.r = self.add_weight(\"r\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        self.bx = self.add_weight(\"bx\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        self.bp = self.add_weight(\"bp\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        \n",
    "        if self.activation == 'kerr':\n",
    "            self.kappa = self.add_weight(\"kappa\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        else:  # cubicphase\n",
    "            self.gamma = self.add_weight(\"gamma\", shape=[1,], initializer=initializer, trainable=True)\n",
    "\n",
    "    @tf.function\n",
    "    def check_norm(self, states):\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.abs(states)**2, axis=1, keepdims=True))\n",
    "        return tf.reduce_all(tf.abs(norm - 1) < self.tol)\n",
    "\n",
    "    @tf.function\n",
    "    def apply_and_check(self, operator, state):\n",
    "        new_state = tf.einsum('bij,bjk->bik', operator, state)\n",
    "        is_valid = self.check_norm(new_state)\n",
    "        return tf.cond(is_valid, lambda: new_state, lambda: state)\n",
    "\n",
    "    @tf.function\n",
    "    def quantum_operation(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        # Create operators\n",
    "        R_tensor_1 = tf.tile(tf.expand_dims(rotation_operator(self.dim, self.theta_1), 0), [batch_size, 1, 1])\n",
    "        S_tensor = tf.tile(tf.expand_dims(squeezing_operator(self.dim, self.r), 0), [batch_size, 1, 1])\n",
    "        R_tensor_2 = tf.tile(tf.expand_dims(rotation_operator(self.dim, self.theta_2), 0), [batch_size, 1, 1])\n",
    "        D_tensor = tf.tile(tf.expand_dims(displacement_operator(self.dim, self.bx, self.bp), 0), [batch_size, 1, 1])\n",
    "        \n",
    "        if self.activation == 'kerr':\n",
    "            A_tensor = tf.tile(tf.expand_dims(kerr_operator(self.dim, self.kappa), 0), [batch_size, 1, 1])\n",
    "        else:  # cubicphase\n",
    "            A_tensor = tf.tile(tf.expand_dims(cubic_phase_operator(self.dim, self.gamma), 0), [batch_size, 1, 1])\n",
    "\n",
    "        # Apply operations and check norm at each step\n",
    "        state = self.apply_and_check(R_tensor_1, inputs)\n",
    "        state = self.apply_and_check(S_tensor, state)\n",
    "        state = self.apply_and_check(R_tensor_2, state)\n",
    "        state = self.apply_and_check(D_tensor, state)\n",
    "        state = self.apply_and_check(A_tensor, state)\n",
    "        return state\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.quantum_operation(inputs)\n",
    "        \n",
    "    \n",
    "# TensorFlow Custom Layer for Quantum Decoding\n",
    "class QDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, **kwargs):\n",
    "        super(QDecoder, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.x_operator = self.build_x_operator()\n",
    "\n",
    "    def build_x_operator(self):\n",
    "        a = annihilation(self.dim)\n",
    "        x_operator = (a + tf.linalg.adjoint(a)) / 2.0\n",
    "        x_operator = tf.expand_dims(x_operator, axis=0)  # Add batch dimension\n",
    "        return x_operator\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        batch_x_operator = tf.tile(self.x_operator, [batch_size, 1, 1])\n",
    "\n",
    "        # Step 1: Compute \\hat{O} | \\psi \\rangle for each state in the batch\n",
    "        operator_applied_state = tf.einsum('bij,bjk->bik', batch_x_operator, inputs)\n",
    "\n",
    "        # Take the complex conjugate of each state and adjust dimensions\n",
    "        conj_inputs = tf.math.conj(inputs)  # Shape: (batch_size, dim, 1)\n",
    "        conj_inputs_adj = tf.transpose(conj_inputs, perm=[0, 2, 1])  # Shape: (batch_size, 1, dim)\n",
    "\n",
    "        # Compute the expectation value (inner product) for each state in the batch\n",
    "        x_expectation = tf.einsum('bij,bjk->bi', conj_inputs_adj, operator_applied_state) \n",
    "        x_expectation = tf.squeeze(x_expectation, axis=-1)\n",
    "\n",
    "        return tf.math.real(x_expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2Score metric wrapper to handle shape issues\n",
    "class R2ScoreWrapper(tf.keras.metrics.R2Score):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.reshape(y_true, [-1, 1])\n",
    "        y_pred = tf.reshape(y_pred, [-1, 1])\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "    \n",
    "\n",
    "# TensorFlow Custom Callback for Progress Bars\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class TrainingProgress(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        epoch += 1  # epochs are zero-indexed in this method\n",
    "        \n",
    "        # Get training loss, validation loss, and learning rate\n",
    "        train_loss = logs.get('loss')\n",
    "        val_loss = logs.get('val_loss')\n",
    "        lr = self.model.optimizer.lr\n",
    "        \n",
    "        # If lr is a callable (LearningRateSchedule), get its current value\n",
    "        if callable(lr):\n",
    "            lr = lr(self.model.optimizer.iterations)\n",
    "        \n",
    "        # Convert lr tensor to float\n",
    "        lr = float(lr)\n",
    "\n",
    "        print(f\"Epoch: {epoch:5d} | LR: {lr:.7f} | Loss: {train_loss:.7f} | Val Loss: {val_loss:.7f}\")\n",
    "\n",
    "        # Every 5 epochs, clear the screen\n",
    "        if epoch % 5 == 0:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "\n",
    "# TensorFlow Custom Callback for Parameter Logging\n",
    "class ParameterLoggingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, fold, function_index, activation, base_dir='Params'):\n",
    "        super(ParameterLoggingCallback, self).__init__()\n",
    "        self.fold = fold\n",
    "        self.function_index = function_index\n",
    "        self.activation = activation\n",
    "        self.base_dir = base_dir\n",
    "        self.params_dir = os.path.join(base_dir, f'Function_{function_index}')\n",
    "        self.filename = os.path.join(self.params_dir, f'parameters_fold_{fold}.csv')\n",
    "        self.epoch = 0\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(self.params_dir, exist_ok=True)\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Count the number of QLayers\n",
    "        self.num_qlayers = sum(1 for layer in self.model.layers if isinstance(layer, QLayer))\n",
    "        \n",
    "        # Create the CSV file for parameters and write the header\n",
    "        with open(self.filename, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            header = ['Epoch']\n",
    "            for i in range(self.num_qlayers):\n",
    "                if self.activation == 'kerr':\n",
    "                    header.extend([f'Layer{i}_theta_1', f'Layer{i}_r', f'Layer{i}_theta_2', \n",
    "                               f'Layer{i}_bx', f'Layer{i}_bp', f'Layer{i}_kappa'])\n",
    "                else:\n",
    "                    header.extend([f'Layer{i}_theta_1', f'Layer{i}_r', f'Layer{i}_theta_2', \n",
    "                               f'Layer{i}_bx', f'Layer{i}_bp', f'Layer{i}_gamma'])\n",
    "            writer.writerow(header)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch += 1\n",
    "        params = []\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, QLayer):\n",
    "                if self.activation == 'kerr':\n",
    "                    layer_params = [\n",
    "                        layer.theta_1.numpy()[0],\n",
    "                        layer.r.numpy()[0],\n",
    "                        layer.theta_2.numpy()[0],\n",
    "                        layer.bx.numpy()[0],\n",
    "                        layer.bp.numpy()[0],\n",
    "                        layer.kappa.numpy()[0]\n",
    "                    ]\n",
    "                else:\n",
    "                    layer_params = [\n",
    "                        layer.theta_1.numpy()[0],\n",
    "                        layer.r.numpy()[0],\n",
    "                        layer.theta_2.numpy()[0],\n",
    "                        layer.bx.numpy()[0],\n",
    "                        layer.bp.numpy()[0],\n",
    "                        layer.gamma.numpy()[0]\n",
    "                    ]    \n",
    "                params.extend(layer_params)\n",
    "        \n",
    "        # Append the parameters to the CSV file\n",
    "        with open(self.filename, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([self.epoch] + params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Custom Callback for Progress Bars\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class TrainingProgress(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        epoch += 1  # epochs are zero-indexed in this method\n",
    "        \n",
    "        # Get training loss, validation loss, and learning rate\n",
    "        train_loss = logs.get('loss')\n",
    "        val_loss = logs.get('val_loss')\n",
    "        lr = self.model.optimizer.lr\n",
    "        \n",
    "        # If lr is a callable (LearningRateSchedule), get its current value\n",
    "        if callable(lr):\n",
    "            lr = lr(self.model.optimizer.iterations)\n",
    "        \n",
    "        # Convert lr tensor to float\n",
    "        lr = float(lr)\n",
    "\n",
    "        print(f\"Epoch: {epoch:5d} | LR: {lr:.7f} | Loss: {train_loss:.7f} | Val Loss: {val_loss:.7f}\")\n",
    "\n",
    "        # Every 5 epochs, clear the screen\n",
    "        if epoch % 5 == 0:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "\n",
    "# TensorFlow Custom Callback for Parameter Logging\n",
    "class ParameterLoggingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, fold, function_index, activation, base_dir='Params'):\n",
    "        super(ParameterLoggingCallback, self).__init__()\n",
    "        self.fold = fold\n",
    "        self.function_index = function_index\n",
    "        self.activation = activation\n",
    "        self.base_dir = base_dir\n",
    "        self.params_dir = os.path.join(base_dir, f'Function_{function_index}')\n",
    "        self.filename = os.path.join(self.params_dir, f'parameters_fold_{fold}.csv')\n",
    "        self.epoch = 0\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(self.params_dir, exist_ok=True)\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Count the number of QLayers\n",
    "        self.num_qlayers = sum(1 for layer in self.model.layers if isinstance(layer, QLayer))\n",
    "        \n",
    "        # Create the CSV file for parameters and write the header\n",
    "        with open(self.filename, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            header = ['Epoch']\n",
    "            for i in range(self.num_qlayers):\n",
    "                if self.activation == 'kerr':\n",
    "                    header.extend([f'Layer{i}_theta_1', f'Layer{i}_r', f'Layer{i}_theta_2', \n",
    "                               f'Layer{i}_bx', f'Layer{i}_bp', f'Layer{i}_kappa'])\n",
    "                else:\n",
    "                    header.extend([f'Layer{i}_theta_1', f'Layer{i}_r', f'Layer{i}_theta_2', \n",
    "                               f'Layer{i}_bx', f'Layer{i}_bp', f'Layer{i}_gamma'])\n",
    "            writer.writerow(header)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch += 1\n",
    "        params = []\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, QLayer):\n",
    "                if self.activation == 'kerr':\n",
    "                    layer_params = [\n",
    "                        layer.theta_1.numpy()[0],\n",
    "                        layer.r.numpy()[0],\n",
    "                        layer.theta_2.numpy()[0],\n",
    "                        layer.bx.numpy()[0],\n",
    "                        layer.bp.numpy()[0],\n",
    "                        layer.kappa.numpy()[0]\n",
    "                    ]\n",
    "                else:\n",
    "                    layer_params = [\n",
    "                        layer.theta_1.numpy()[0],\n",
    "                        layer.r.numpy()[0],\n",
    "                        layer.theta_2.numpy()[0],\n",
    "                        layer.bx.numpy()[0],\n",
    "                        layer.bp.numpy()[0],\n",
    "                        layer.gamma.numpy()[0]\n",
    "                    ]    \n",
    "                params.extend(layer_params)\n",
    "        \n",
    "        # Append the parameters to the CSV file\n",
    "        with open(self.filename, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([self.epoch] + params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfenv)",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
